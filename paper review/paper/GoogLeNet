[paper review]

**GoogLeNet** aka Inception

-----------------------------------------------------------

Going Deeper with Convolutions(2014)
by Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich

Abstract
목적: 네트워크의 컴퓨팅 리소스를 향상시키는 구조를 설계하여, 컴퓨팅 리소스를 유지하면서 네트워크의 depth 와 width를 증가시킬 수 있도록 함
방법: 
- 아키텍쳐는 헵의 이론과 멀티스케일 처리의 직관에 기반함
  - 헵의 이론: 연결주의(connectionism) ; 인간의 마음 현상(인간의 마음, 인지적 능력)이란 단순하고 병렬적인 계산 요소의 대규모로 연결망으로 봄
    - 인간의 뉴런의 작용이 어떻게 학습과 같은 과정에 기여하는가? -> 함께 발화하는 세포들은 연결된다. 학습은 뉴런들 사이에 새로운 연결을 만드는 활동이며, 기억은 이러한 연결을 강화하고 유지하는 장치이다.
    - 두 뉴런이 서로 반복적이고 지속적으로 점화하여 어느 한쪽 또는 양쪽 모두에 어떤 변화를 만든다 -> 상호간의 점화의 효율(=가중치의 변화)이 점점 커짐
    - 시냅스를 통한 신경세포의 상호작용이 증가할수록 두 시냅스가 더 견고하게 강화됨. 상호작용이 감소하면 시냅스가 약해져서 신경회로를 사용하지 않게 됨.
    - 새로운 것을 배울 때, 뇌 기억 시스템은 기존의 신경망에 새로운 연결을 추가함. 
    - 새로운 정보와 연결된 회로는 강화되며(가중치 증가) 이 때 활성화 되지 않는 뉴런들은 연결이 약해지고(가중치 감소), 이런 과정의 반복을 통해 기억이 형성
    - cell assembly
- 22개 층으로 된 심층 네트워크 구성
결과:ILSVRC2014 에 제출 classification and detection



1. Introduction
- AlexNet(2012) 의 아키텍쳐보다 12배 적은 파라미터를 사용하면서도 훨씬 더 정확한 모델
  - AlexNet 60M 
  - VGGNet 110M
  - GoogLeNet 4M
- detection에서의 큰 성과
- 알고리즘의 효율성, 특히 전력 및 메모리 사용량에서 효과적 
- 더 깊이 들어간다는 의미로 CODE NAME : Inception 


2. Related Work
- 최근의 추세는, 과적합 문제를 해결하기 위해 Dropout 을 사용하며 레이어 수와 레이어 크기를 늘린다.
- MaxPooling layer 가 정확한 spatial information 손실을 초래한다는 단점이 있으나, AlexNet 등의 성능이 좋음
- GoogLeNet 모델은 Inception layer가 여러번 반복되어 22개의 층을 쌓음
- Network-in-Network 에서 conv 의 곱,합 연산은 선형적이므로 비선형적인 특징들에는 한계를 가짐 -> MLP(1*1 convolution layer) 추가하여 비선형적 특징을 추출
- 1*1 conv layer 사용 목적
  - 1. 네트워크 크기를 제한하는 컴퓨팅 병목 제거를 위한 차원축소 모듈로 사용
  - 2. 성능 저하 없이 네트워크의 depth 뿐 아니라 width 도 늘림
- 객체 탐지를 위한 선도적인 접근 방식 R-CNN(regions with Convolution Neural Net-work)
  - 범주에 구애 받지 않는 방식; color, sub-pixel 과 같은 low-level 단서를 이용 -> object proposals -> CNN 을 이용하여 해당 위치의 object 의 category를 정함

3. Motivation and High Level Considerations
- 심층신경망의 성능을 개선하는 가장 간단한 방법은 크기를 늘리는 것; depth(level), width(level의 단위 수) 를 늘림
  - 대량의 레이블이 지정된 학습데이터를 사용할 때 더 높은 품질의 모델을 쉽고 안전하게 학습하는 방법
  - 크기가 클 수록 더 많은 parameter 필요
  - 과적합의 위험
  - 네트워크의 크기가 균일하게 증가하면 컴퓨팅 리소스 사용이 급격히 증가
    - e.g. conv layer 2개가 연결된 경우, 필터 수가 균이라게 증가하면 계산이 4배로 증가
    - e.g. 대부분의 가중치가 0에 가까워지는 경우, 컴퓨팅 리소스가 낭비됨
- 단점 해결을 위해 FC 및 conv layer -> Sparsely Connected architecture 로 변경 -> 헵의 규칙 적용
  - Sparsely Connectivity
    - input node 사이에 correlation 이 높을 때 output 의 서로 연관관계가 높으면 둘을 연결시켜주되, 나머지는 연결시키지 않는다.
    - 이렇게 연결하면 cluster가 생기고 sparse 한 connection의 architecture 가 된다. 
    - Sparse Neural Network 의 경우, 많은 weight가 0 이 되므로 연산이 줄어들어 데이터 연산을 효율적으로 만듬 
    - 예상과 달리 실제 Dense matrix 연산보다 Sparse matrix 연산이 더 큰 Computational resourse 를 사용한다. 

4. Architectural Details
- convolution vision network 에서 optimal local sparse 구조를 근사화 하는 방법 기반
- convolution building blocks



5. GoogLeNet



6. Training Methodology



7. ILSVRC 2014 Classification Challenge Setup and Results



8. ILSVRC 2014 Detection Challenge Setup and Results



9. Conclusions



10. Acknowledgements
